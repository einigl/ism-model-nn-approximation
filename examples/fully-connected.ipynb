{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron\n",
    "\n",
    "This notebook illustrates the creation and the training of a regular multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"\"), \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from nnbma.networks import FullyConnected\n",
    "from nnbma.dataset import RegressionDataset\n",
    "from nnbma.learning import learning_procedure, LearningParameters\n",
    "\n",
    "from functions import Fexample as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical function\n",
    "\n",
    "In the following cell, we load and instantiate a vectorial function $f$ implemented as a PyTorch `Module`. For more details on the implementation, see `functions.py`. You can implement your own by following the model.\n",
    "\n",
    "The function is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$$\\left(\\begin{array}{c} t_1\\\\ t_2 \\end{array}\\right) \\longmapsto \\left(\\begin{array}{c} t_1+2t_2\\\\ t_1^2\\\\ t_1t_2^2 \\end{array}\\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = F()\n",
    "md(F.latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the architecture\n",
    "\n",
    "The multilayer perceptron is historically the most common architecture. Every layer is fully connected to the previous one (that why is it often called \"fully connected neural network\").\n",
    "\n",
    "Below, an example from the [related paper](https://www.aanda.org/articles/aa/full_html/2023/10/aa47074-23/aa47074-23.html) for 2 inputs, 10 outputs and 2 hidden layers of respective sizes 4 and 8:\n",
    "\n",
    "<img src=\"img/annotated-chain-nn.png\" width=500>\n",
    "\n",
    "The transition between two layer is composed of a matrix product and an activation function such as $\\mathrm{ReLU}$, $\\mathrm{tanh}$ or any of their numerous variants.\n",
    "\n",
    "The parameters that you can tuned in order to improve your model are the following:\n",
    "\n",
    "- the __layers sizes__ (number of layer and number of neurons of each of them)\n",
    "- the __activation function__ (for simplicity sake, in this implementation the same function is used for every layers)\n",
    "- the use of a __batch normalization__ or not\n",
    "\n",
    "You can also change the loss function that you use (see the section \"Training procedure\").\n",
    "\n",
    "With a `FullyConnected` network, as well than for any network that inherits from `NeuralNetwork`, you can specify the input and output features names, the device (cpu or gpu, if available) that you want to use.\n",
    "\n",
    "Lastly, you can also specifiy whether you want the last layer to be restrictable (see the `restrictable-layer.ipynb` notebook in this topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers: 3\n",
      "Layers sizes: [2, 50, 50, 3]\n",
      "Number of trainable weights: 2,853 (11.41 kB)\n"
     ]
    }
   ],
   "source": [
    "layers_sizes = [f.n_inputs, 50, 50, f.n_outputs]  # Can be modified\n",
    "activation = nn.ELU()\n",
    "\n",
    "net = FullyConnected(layers_sizes, activation)\n",
    "\n",
    "print(f\"Number of hidden layers: {len(layers_sizes)-1}\")\n",
    "print(f\"Layers sizes: {layers_sizes}\")\n",
    "print(\n",
    "    f\"Number of trainable weights: {net.count_parameters():,} ({net.count_bytes(display=True)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries: 8,000\n",
      "Number of testing entries: 2,000\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10_000\n",
    "test_frac = 0.20\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(0, 1, size=(n_samples, F.n_inputs)).astype(\"float32\")\n",
    "Y = f(X)\n",
    "\n",
    "X_train, X_test = X[round(test_frac * n_samples) :], X[: round(test_frac * n_samples)]\n",
    "Y_train, Y_test = Y[round(test_frac * n_samples) :], Y[: round(test_frac * n_samples)]\n",
    "\n",
    "train_dataset = RegressionDataset(X_train, Y_train)\n",
    "test_dataset = RegressionDataset(X_test, Y_test)\n",
    "\n",
    "print(f\"Number of training entries: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of testing entries: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epochs = 100\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_params = LearningParameters(loss, epochs, batch_size, optimizer)\n",
    "\n",
    "results = learning_procedure(\n",
    "    net,\n",
    "    (train_dataset, test_dataset),\n",
    "    learning_params,\n",
    "    val_frac=test_frac,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_hat: np.ndarray, y: np.ndarray):\n",
    "    return np.mean((y_hat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss over training set: 8.36e-04\n",
      "Loss over testing set: 2.94e-03\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss over training set: {metric(net(X_train), Y_train):.2e}\")\n",
    "print(f\"Loss over testing set: {metric(net(X_test), Y_test):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnbma-aChir395-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
