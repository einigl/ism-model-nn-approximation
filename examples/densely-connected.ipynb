{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense multilayer perceptron\n",
    "\n",
    "This notebook illustrates the creation and the training of a densely connected multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"\"), \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from nnbma.networks import DenselyConnected\n",
    "from nnbma.dataset import RegressionDataset\n",
    "from nnbma.learning import learning_procedure, LearningParameters\n",
    "\n",
    "from functions import Fexample as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical function\n",
    "\n",
    "In the following cell, we load and instantiate a vectorial function $f$ implemented as a PyTorch `Module`. For more details on the implementation, see `functions.py`. You can implement your own by following the model.\n",
    "\n",
    "The function is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$$\\left(\\begin{array}{c} t_1\\\\ t_2 \\end{array}\\right) \\longmapsto \\left(\\begin{array}{c} t_1+2t_2\\\\ t_1^2\\\\ t_1t_2^2 \\end{array}\\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = F()\n",
    "md(F.latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the architecture\n",
    "\n",
    "The densely connected multilayer perceptron is a novative architecture inspired by the convolutional architecture \"DenseNet\" mainly use for image recognition. This is a multilayer perceptron that concatenate at each layer every previous intermediate result. The fraction of new intermediate result created at each layer is called \"growing factor\".\n",
    "\n",
    "Below, an example from the [related paper](https://www.aanda.org/articles/aa/full_html/2023/10/aa47074-23/aa47074-23.html) for 2 inputs, 10 outputs and 2 hidden layers of with a growing factor of 1:\n",
    "\n",
    "<img src=\"img/annotated-dense-nn.png\" width=500>\n",
    "\n",
    "The interest of this architecture is that you can stack a lot of linear layers without experimenting issues such as vanishing gradient. Moreover, if some output features are proportional with some intermediate results, they will be propagated without forcing the network to approach the identity function.\n",
    "\n",
    "The parameters that you can tuned in order to improve your model are the following:\n",
    "\n",
    "- the __number of hidden layers__\n",
    "- the __growing factor__ (the lower is the growing factor, the higher the number of layer will be for the same number of parameter)\n",
    "- the __activation function__ (for simplicity sake, in this implementation the same function is used for every layers)\n",
    "- the use of a __batch normalization__ or not\n",
    "\n",
    "You can also change the loss function that you use (see the section \"Training procedure\").\n",
    "\n",
    "With a `DenselyConnected` network, as well than for any network that inherits from `NeuralNetwork`, you can specify the input and output features names, the device (cpu or gpu, if available) that you want to use.\n",
    "\n",
    "Lastly, you can also specifiy whether you want the last layer to be restrictable (see the `restrictable-layer.ipynb` notebook in this topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers: 10\n",
      "Layers sizes: [2, 1, 2, 3, 4, 6, 9, 14, 21, 31, 3]\n",
      "Number of trainable weights: 3,823 (15.29 kB)\n"
     ]
    }
   ],
   "source": [
    "n_layers = 10\n",
    "growing_factor = 0.5\n",
    "activation = nn.ELU()\n",
    "\n",
    "net = DenselyConnected(f.n_inputs, f.n_outputs, n_layers, growing_factor, activation)\n",
    "\n",
    "print(f\"Number of hidden layers: {len(net.layers_sizes)-1}\")\n",
    "print(f\"Layers sizes: {net.layers_sizes}\")\n",
    "print(\n",
    "    f\"Number of trainable weights: {net.count_parameters():,} ({net.count_bytes(display=True)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training entries: 8,000\n",
      "Number of testing entries: 2,000\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10_000\n",
    "test_frac = 0.20\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(0, 1, size=(n_samples, F.n_inputs)).astype(\"float32\")\n",
    "Y = f(X)\n",
    "\n",
    "X_train, X_test = X[round(test_frac * n_samples) :], X[: round(test_frac * n_samples)]\n",
    "Y_train, Y_test = Y[round(test_frac * n_samples) :], Y[: round(test_frac * n_samples)]\n",
    "\n",
    "train_dataset = RegressionDataset(X_train, Y_train)\n",
    "test_dataset = RegressionDataset(X_test, Y_test)\n",
    "\n",
    "print(f\"Number of training entries: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of testing entries: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epochs = 100\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_params = LearningParameters(loss, epochs, batch_size, optimizer)\n",
    "\n",
    "results = learning_procedure(\n",
    "    net,\n",
    "    (train_dataset, test_dataset),\n",
    "    learning_params,\n",
    "    val_frac=test_frac,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_hat: np.ndarray, y: np.ndarray):\n",
    "    return np.mean((y_hat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss over training set: 3.82e-04\n",
      "Loss over testing set: 1.81e-03\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss over training set: {metric(net(X_train), Y_train):.2e}\")\n",
    "print(f\"Loss over testing set: {metric(net(X_test), Y_test):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnbma-aChir395-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
