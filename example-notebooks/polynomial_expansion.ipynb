{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial expansion of inputs\n",
    "\n",
    "In this notebook, we introduce the `PolynomialExpansion` module which is able to create polynomial features of inputs in a differentiable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "\n",
    "import torch\n",
    "\n",
    "from nnbma.networks import FullyConnected, PolynomialNetwork\n",
    "from nnbma.layers import PolynomialExpansion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PolynomialExpansion` module\n",
    "\n",
    "`PolynomialExpansion` is a torch `Module` that creates all possible (non-constant) monomial from a set of inputs. For instance, for `degree=2`, we have:\n",
    "\n",
    "$ \\mathrm{poly}((x_1,\\,x_2,\\,x_3)) = (x_1,\\,x_2,\\,x_3,\\,x_1^2,\\,x_1x_2,\\,x_1x_3,\\,x_2^2,\\,x_2x_3,\\,x_3^2) $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the corresponding expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([2., 3., 5.])\n",
      "Output: tensor([ 2.,  3.,  5.,  4.,  6., 10.,  9., 15., 25.])\n"
     ]
    }
   ],
   "source": [
    "input_features = 3\n",
    "order = 2\n",
    "\n",
    "layer = PolynomialExpansion(input_features, order)\n",
    "\n",
    "x = torch.tensor([2., 3., 5.]) # Must have x.shape[-1] = input_features\n",
    "print(\"Input:\", x)\n",
    "\n",
    "y = layer(x)\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any modules from this package, it works with batched inputs along the first axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[2., 3., 5.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 0.],\n",
      "         [0., 2., 3.]]])\n",
      "Output: tensor([[[ 2.,  3.,  5.,  4.,  6., 10.,  9., 15., 25.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  2.,  3.,  0.,  0.,  0.,  4.,  6.,  9.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [\n",
    "        [2., 3., 5.],\n",
    "        [1., 1., 1.],\n",
    "    ], [\n",
    "        [1., 0., 0.],\n",
    "        [0., 2., 3.],\n",
    "    ]\n",
    "]) # Must have x.shape[-1] = input_features\n",
    "print(\"Input:\", x)\n",
    "\n",
    "y = layer(x)\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to its classical use in preprocessing, this expansion is completely differentiable with respect to its inputs, so that it can be integrated into a neural network (and not placed before, a situation where the derivation of the network with respect to the inputs would be performed with respect to the developed inputs, and not with respect to the real inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([2., 3., 5.], requires_grad=True)\n",
      "Output gradient: tensor([ 2.,  3.,  5.,  4.,  6., 10.,  9., 15., 25.],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2., 3., 5.], requires_grad=True) # Must have x.shape[-1] = input_features\n",
    "print(\"Input:\", x)\n",
    "\n",
    "y = layer(x)\n",
    "print(\"Output gradient:\", y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PolynomialNetwork` module\n",
    "\n",
    "`PolynomialNetwork` is a convenience class that allows to integrate a `PolynomialLayer` at the input of a network inheriting from `NeuralNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet = FullyConnected(\n",
    "    [PolynomialExpansion.expanded_features(order, input_features), 10, 10, 1], # expanded_features allow to anticipate the number of polynomial features that the subnetwork will have as input, depending on the number of real input features and the max order.\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "net = PolynomialNetwork(\n",
    "    input_features,\n",
    "    order,\n",
    "    subnet,\n",
    ")\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([2., 3., 5.], requires_grad=True)\n",
      "Output gradient: tensor([0.0873], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2., 3., 5.], requires_grad=True) # Must have x.shape[-1] = input_features\n",
    "print(\"Input:\", x)\n",
    "\n",
    "y = net(x)\n",
    "print(\"Output gradient:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of polynomial features\n",
    "\n",
    "The outputs of a `PolynomialLayer` are not standardized, even for standardized inputs. For example, if an input feature $x$ is standardized (meaning that $\\mu_{x}=0$ and $\\sigma_{x}=1$), then $x^2$ is no longer standardized since $\\mu_{x^2}=1$ and generally $\\sigma_{x_2}\\neq1$.\n",
    "\n",
    "There is no analytical way of calculating the moments of polynomial features, as this depends on the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.normal(0, torch.ones(100, 2))\n",
    "order = 2\n",
    "\n",
    "layer = PolynomialExpansion(x.size(-1), order)\n",
    "print(layer.means, layer.stds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0141, 0.0136]) tensor([ 1.4075e-02,  1.3608e-02,  1.3235e-02, -6.1836e-05,  9.3748e-03])\n"
     ]
    }
   ],
   "source": [
    "print(x.mean(dim=0), layer(x).mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 1.4075e-02,  1.3608e-02,  1.3235e-02, -6.1836e-05,  9.3748e-03])\n",
      "Parameter containing:\n",
      "tensor([0.1142, 0.0959, 0.0212, 0.0111, 0.0129])\n"
     ]
    }
   ],
   "source": [
    "layer.update_standardization(x, reset=True)\n",
    "print(layer.means, layer.stds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standardization can also be done by batch in case of large datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0241, 0.0224, 0.0118, 0.0014, 0.0101])\n",
      "Parameter containing:\n",
      "tensor([0.1058, 0.0980, 0.0140, 0.0113, 0.0143])\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 1.4075e-02,  1.3608e-02,  1.3235e-02, -6.1836e-05,  9.3748e-03])\n",
      "Parameter containing:\n",
      "tensor([0.1142, 0.0959, 0.0212, 0.0111, 0.0129])\n"
     ]
    }
   ],
   "source": [
    "layer.update_standardization(x[:50], reset=True)\n",
    "print(layer.means, layer.stds, \"\", sep=\"\\n\")\n",
    "layer.update_standardization(x[50:])\n",
    "print(layer.means, layer.stds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a convenience method of `PolynomialNetwork` also called `update_standardization`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
