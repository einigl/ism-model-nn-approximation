{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networjs based approximation of an analytical function\n",
    "\n",
    "In this notebooj, we will show how to train a neural networj to approximate a jnwon analytical function. The same procedure can be used to approximate a unjnown function, for example using real world data.\n",
    "\n",
    "By convenience, the neural network implementation of this package handle the conversion between NumPy and PyTorch so we won't directly manipulate any PyTorch `Tensor` in this notebook.\n",
    "\n",
    "The analytical function and the network will be saved in the `out-training` directory, so this notebook is also an introduction to `.save` and `.load` methods of the abstract class `NeuralNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from nnbma.networks import FullyConnected\n",
    "from nnbma.dataset import RegressionDataset\n",
    "from nnbma.learning import learning_procedure, LearningParameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the analytical function\n",
    "\n",
    "In the following cell, we load and instantiate the vectorial function $ f: \\left(\\begin{array}{c} t_1\\\\ t_2 \\end{array}\\right) \\longmapsto \\left(\\begin{array}{c} t_1+2t_2\\\\ t_1^2\\\\ t_1t_2^2 \\end{array}\\right) $ implemented as a PyTorch `Module`. For more details on the implementation, see `functions.py`. You can implemented your own by following the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import F\n",
    "\n",
    "f = F()\n",
    "\n",
    "ranges = [(-2., 2.), (-1., 1.)] # Range of values of all inputs of f. Can be modified but must be consistent with the imported function.\n",
    "\n",
    "n_points = 100 # Number of points needed to plot a profile\n",
    "\n",
    "default_values = [0., 0.] # Default values of all inputs of f when plotting a profile. Can be modified but must be consistent with the imported function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, consider that `f` is just a function that maps vectors into other vectors (but we will see in other notebooks that it is actually more than that, for example it can be derived automatically or integrated into a neural network if desired).\n",
    "\n",
    "For instance, we can plot some profiles of `f`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of free input, the others being restricted to their default value.\n",
    "input_to_plot = 1 # Must be between 1 and f.n_inputs\n",
    "\n",
    "x = np.array(default_values) * np.ones((n_points, f.n_inputs), dtype=\"float\")\n",
    "x[:, input_to_plot-1] = np.linspace(*ranges[input_to_plot-1], n_points)\n",
    "print(\"x.shape:\", x.shape)\n",
    "\n",
    "y = f(x)\n",
    "print(\"y.shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(f.n_outputs*6.4, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "\n",
    "    plt.plot(x[:, input_to_plot-1], y[:, j])\n",
    "\n",
    "    plt.xlabel(f\"$x_{input_to_plot}$\")\n",
    "    plt.ylabel(f\"$f_{j+1}$\")\n",
    "    plt.title(f\"$f_{j+1}$ vs $x_{input_to_plot}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a bivariate plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of free inputs, the others being restricted to their default value.\n",
    "inputs_to_plot = (1, 2) # Must be between 1 and f.n_inputs\n",
    "\n",
    "X = np.dstack(np.meshgrid(\n",
    "    *[np.linspace(*ranges[i], n_points) if i+1 in inputs_to_plot else default_values[i] * np.ones(n_points) for i in range(f.n_inputs)]\n",
    "))\n",
    "print(\"X.shape:\", X.shape)\n",
    "\n",
    "Y = f(X)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.imshow(\n",
    "        Y[..., j], cmap=\"jet\", aspect=\"auto\", extent=[\n",
    "            ranges[inputs_to_plot[0]-1][0], ranges[inputs_to_plot[0]-1][1],\n",
    "            ranges[inputs_to_plot[1]-1][0], ranges[inputs_to_plot[1]-1][1],\n",
    "        ],\n",
    "    )\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.xlabel(f\"$x_{inputs_to_plot[0]}$\")\n",
    "    plt.ylabel(f\"$x_{inputs_to_plot[1]}$\")\n",
    "    plt.title(f\"$f_{j+1}$ vs $x_{inputs_to_plot[0]}$ and $x_{inputs_to_plot[1]}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of a neural network\n",
    "\n",
    "We will create a fully connected neural network. To do so, we have to define, some hyperparameters:\n",
    "* the number of hidden layers\n",
    "* their respective number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_sizes = [f.n_inputs, 100, 100, f.n_outputs]\n",
    "activation = nn.GELU()\n",
    "\n",
    "net_0 = FullyConnected(\n",
    "    layers_sizes,\n",
    "    activation,\n",
    ")\n",
    "\n",
    "net = net_0.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are initialized randomly so, before any training, we obtain plots like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_net_0 = net_0(x)\n",
    "\n",
    "plt.figure(figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.plot(x[:, input_to_plot-1], y_net_0[:, j])\n",
    "\n",
    "    plt.xlabel(f\"$x_{input_to_plot}$\")\n",
    "    plt.ylabel(f\"$\\\\hat{{f}}_{j+1}$\")\n",
    "    plt.title(f\"$\\\\hat{{f}}_{j+1}$ vs $x_{input_to_plot}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "Y_net_0 = net_0(X)\n",
    "\n",
    "plt.figure(dpi=125, figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.imshow(\n",
    "        Y_net_0[..., j], cmap=\"jet\", aspect=\"auto\", extent=[\n",
    "            ranges[inputs_to_plot[0]-1][0], ranges[inputs_to_plot[0]-1][1],\n",
    "            ranges[inputs_to_plot[1]-1][0], ranges[inputs_to_plot[1]-1][1],\n",
    "        ],\n",
    "    )\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.xlabel(f\"$x_{inputs_to_plot[0]}$\")\n",
    "    plt.ylabel(f\"$x_{inputs_to_plot[1]}$\")\n",
    "    plt.title(f\"$\\\\hat{{f}}_{j+1}$ vs $x_{inputs_to_plot[0]}$ and $x_{inputs_to_plot[1]}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the network\n",
    "\n",
    "We will train this network in order to approximate the previous function. To do so, we generate a dataset of randomly distributed points (not a grid like the one used to draw the figures)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell controls the size of the whole dataset, and which fraction of it will be used to test the model instead of improve its learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 20_000\n",
    "test_frac = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.column_stack(\n",
    "    [np.random.uniform(*ranges[k], n_data) for k in range(f.n_inputs)]\n",
    ")\n",
    "output_data = f(input_data)\n",
    "\n",
    "dataset = RegressionDataset(input_data, output_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some parameters to control the learning procedure:\n",
    "* the number of epochs\n",
    "* the batch size\n",
    "* the loss function\n",
    "* the learning rate\n",
    "* the optimizer\n",
    "* the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epochs = 100\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# Scheduler\n",
    "gamma = (1e-2)**(1/epochs)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will launch the training procedure. It will modify the weights of the network in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = LearningParameters(loss, epochs, batch_size, optimizer, scheduler)\n",
    "\n",
    "results = learning_procedure(\n",
    "    net,\n",
    "    dataset,\n",
    "    learning_params,\n",
    "    val_frac=test_frac,\n",
    ")\n",
    "\n",
    "print(\"Results keys:\", list(results.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the evolution of both train and test loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4, 0.7*4.8))\n",
    "\n",
    "plt.plot(results[\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(results[\"val_loss\"], label=\"Test loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now diplay the new plots with the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_net = net(x)\n",
    "\n",
    "plt.figure(figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.plot(x[:, input_to_plot-1], y_net[:, j])\n",
    "\n",
    "    plt.xlabel(f\"$x_{input_to_plot}$\")\n",
    "    plt.ylabel(f\"$\\\\hat{{f}}_{j+1}$\")\n",
    "    plt.title(f\"$\\\\hat{{f}}_{j+1}$ vs $x_{input_to_plot}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "Y_net = net(X)\n",
    "\n",
    "plt.figure(dpi=125, figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.imshow(\n",
    "        Y_net[..., j], cmap=\"jet\", aspect=\"auto\", extent=[\n",
    "            ranges[inputs_to_plot[0]-1][0], ranges[inputs_to_plot[0]-1][1],\n",
    "            ranges[inputs_to_plot[1]-1][0], ranges[inputs_to_plot[1]-1][1],\n",
    "        ],\n",
    "    )\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.xlabel(f\"$x_{inputs_to_plot[0]}$\")\n",
    "    plt.ylabel(f\"$x_{inputs_to_plot[1]}$\")\n",
    "    plt.title(f\"$\\\\hat{{f}}_{j+1}$ vs $x_{inputs_to_plot[0]}$ and $x_{inputs_to_plot[1]}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the differences with the original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_net = y_net - y\n",
    "\n",
    "plt.figure(figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    vmax = np.abs(error_net[:, j]).max()\n",
    "\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.plot(x[:, input_to_plot-1], error_net[:, j])\n",
    "    plt.ylim([-1.1*vmax, 1.1*vmax])\n",
    "\n",
    "    plt.xlabel(f\"$x_{input_to_plot}$\")\n",
    "    plt.ylabel(f\"$\\\\hat{{f}}_{j+1}$\")\n",
    "    plt.title(f\"Error of $\\\\hat{{f}}_{j+1}$ vs $x_{input_to_plot}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "Error_net = Y_net - Y\n",
    "\n",
    "plt.figure(dpi=125, figsize=(6.4*f.n_outputs, 4.8))\n",
    "\n",
    "for j in range(f.n_outputs):\n",
    "    vmax = np.abs(Error_net[..., j]).max()\n",
    "\n",
    "    plt.subplot(1, f.n_outputs, j+1)\n",
    "    plt.imshow(\n",
    "        Error_net[..., j], cmap=\"seismic\", aspect=\"auto\", extent=[\n",
    "            ranges[inputs_to_plot[0]-1][0], ranges[inputs_to_plot[0]-1][1],\n",
    "            ranges[inputs_to_plot[1]-1][0], ranges[inputs_to_plot[1]-1][1],\n",
    "        ], vmin=-vmax, vmax=vmax,\n",
    "    )\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.xlabel(f\"$x_{inputs_to_plot[0]}$\")\n",
    "    plt.ylabel(f\"$x_{inputs_to_plot[1]}$\")\n",
    "    plt.title(f\"Error of $\\\\hat{{f}}_{j+1}$ vs $x_{inputs_to_plot[0]}$ and $x_{inputs_to_plot[1]}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save of the network\n",
    "\n",
    "In this last part, we will show how to save and load a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.splitext(os.path.abspath(''))[0], \"out-training\")\n",
    "\n",
    "net.save(\"network\", path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_net = FullyConnected.load(\"network\", path)\n",
    "print(loaded_net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the loaded network has the same weights than the one that have been trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference of outputs between net and loaded_net:\", np.mean(loaded_net(x)-net(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
