{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks auto-differentiation using PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.func import jacrev, jacfwd, hessian, vmap\n",
    "from torch.nn import ELU\n",
    "\n",
    "from nnbma.networks import FullyConnected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Differentiation of an analytical function\n",
    "\n",
    "In this part, we will implement in PyTorch the following vectorial function\n",
    "\n",
    "$ f: \\left(\\begin{array}{c} t_1\\\\ t_2 \\end{array}\\right) \\longmapsto \\left(\\begin{array}{c} t_1+t_2\\\\ t_1^2+t_2^2\\\\ t_1^3 + t_2^3 \\end{array}\\right) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = 2, 3\n",
    "\n",
    "def f(t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the (t1, t2) -> (t1+t1, t1^2+t2^2, t1^3+t2^3) function in PyTorch.\n",
    "    This function takes batched inputs, so the inputs must have a shape [N, 2] where N is the batch size i.e. the number of inputs that are computed simultaneously.\n",
    "    The output is of shape [N, 3].\n",
    "    \"\"\"\n",
    "    if t.ndim != 2:\n",
    "        raise ValueError(f\"t must have 2 dimensions, not {t.ndim}\")\n",
    "    if t.shape[1] != n_inputs:\n",
    "        raise ValueError(f\"t.shape[1] must be {n_inputs}, not {t.shape[1]}\")\n",
    "    return torch.vstack((t.sum(1), (t**2).sum(1), (t**3).sum(1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can plot a profile of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a bivariate plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Differentiation of a neural network\n",
    "\n",
    "We will train a neural network to approximate the previous function. After that, we will compute the first and second derivatives of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test dataset\n",
    "\n",
    "n_data = 1_000\n",
    "test_frac = 0.20\n",
    "\n",
    "a, b = -2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.uniform(a, b, size=(n_data, n_inputs))\n",
    "outputs = f(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_sizes = [n_inputs, 1000, 1000, 1000, n_outputs]\n",
    "activation = ELU()\n",
    "\n",
    "model = FullyConnected(layers_sizes, activation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
